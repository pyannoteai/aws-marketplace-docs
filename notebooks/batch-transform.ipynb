{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy pyannoteAI Diarization Model Package from AWS Marketplace \n",
    "\n",
    "The Speaker Diarization API enables accurate segmentation of audio recordings by detecting and labeling individual speakers across time. Designed for seamless integration into transcription pipelines, media workflows, and audio analytics systems, it supports a wide range of formats including WAV, MP3, FLAC, and OGG. The service is language-agnostic and works across diverse audio sourcecalls, meetings, interviews, podcasts, and more. With built-in support for mono and stereo channels, varying sample rates, and flexible input options it can be deployed in batch or near-real-time use cases. Key features include automatic speaker count estimation, precise time-stamped speaker labeling, and detection of overlapping speech. Outputs are returned in structured JSON for easy integration with transcription engines, search indexes, or business intelligence tools. Whether you are enriching speech-to-text transcripts, analyzing call center performance, or processing long-form media, this API improves clarity, organization, and data usability.\n",
    "\n",
    "This sample notebook shows you how to deploy the **pyannoteAI Diarization Model** using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This reference notebook cannot run unless you make the suggested changes in the notebook.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements that render correctly in the Jupyter interface. Open it from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that the IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions, and you have the authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. your AWS account has a **pyannoteAI Diarization Model** subscription. If so, skip the step: [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create a model and perform batch inference](#2-perform-batch-inference)\n",
    "    1. [Create a model](#a-create-a-model)\n",
    "    2. [Run Batch Transform Job](#b-run-batch-transform-job)\n",
    "3. [Cleanup](#3.-Cleanup)\n",
    "\n",
    "We recommend using **ml.g4dn.xlarge** instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the model package listing page **pyannoteAI Diarization Model**.\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agree with EULA, pricing, and support terms. \n",
    "1. Once you click on the **Continue to configuration** button and choose a **region**, you will see a **Product ARN** displayed. This is the model package ARN that you need to specify while creating a deployable model using `boto3`. Copy the ARN corresponding to your region and specify it in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is sample ARN please replace with the subscribed ARN.\n",
    "model_package_arn = \"arn:aws:sagemaker:{zone}:{account_id}:model-package/xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sage.Session()\n",
    "runtime = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will perform batch inference using multiple input payloads together. If you are not familiar with batch transform, and want to learn more, see these links:\n",
    "1. [How it works](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-batch-transform.html)\n",
    "2. [How to run a batch transform job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'sample-s3-bucket-for-diarization-test'\n",
    "\n",
    "# Set the local path of the folder where sample audio files are kept\n",
    "sample_audio_input = '../example-input.json'\n",
    "\n",
    "# Set the name of your S3 bucket and the name of the folder you want to create\n",
    "input_folder_name = 'input'\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Create the S3 bucket\n",
    "s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "# Create the folder in the S3 bucket\n",
    "s3.put_object(Bucket=bucket_name, Key=input_folder_name + '/')\n",
    "\n",
    "# Upload file to the folder in the S3 bucket\n",
    "s3.upload_file(sample_audio_input, bucket_name, input_folder_name + '/example-input.json')\n",
    "\n",
    "print(\"Transform input uploaded..\")\n",
    "#Display the content of S3 Bucket\n",
    "# List the objects in the bucket\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "for obj in response['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelName = \"pyannoteai-diarization-batch-inference-model\"\n",
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "cm_res_end = sm_boto3.create_model(\n",
    "    ModelName=ModelName,  # name the of the model does not need to be the same as the image repo\n",
    "    Containers=[\n",
    "        {            \n",
    "            'ModelPackageName': model_package_arn\n",
    "        },\n",
    "    ],    \n",
    "    ExecutionRoleArn=role,\n",
    "    EnableNetworkIsolation=True,\n",
    ")\n",
    "print(\"Model Arn: \" + cm_res_end['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Run Batch Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the batch-transform job\n",
    "transform_job_name = \"pyannoteai-diarization-batch-inference\" + time.strftime(\"-%Y%m%d%H%M\", time.gmtime())\n",
    "\n",
    "response_create_job = sm_boto3.create_transform_job(\n",
    "    TransformJobName=transform_job_name,\n",
    "    ModelName=ModelName,\n",
    "    MaxConcurrentTransforms=1,\n",
    "    #InstanceCount=1\n",
    "    MaxPayloadInMB=50,\n",
    "    TransformInput={\n",
    "        'DataSource': {\n",
    "            'S3DataSource': {\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3Uri': \"s3://\"+bucket_name+\"/input/\"    # Input Folder\n",
    "            }\n",
    "        },\n",
    "        'ContentType': \"application/json\"\n",
    "    },\n",
    "    TransformOutput={\n",
    "        'S3OutputPath': \"s3://\"+bucket_name+\"/output/\", #output folder\n",
    "        'Accept': 'application/json',\n",
    "        'AssembleWith': 'Line',        \n",
    "    },\n",
    "    TransformResources={\n",
    "        'InstanceType': 'ml.g4dn.xlarge',\n",
    "        'InstanceCount': 1      \n",
    "    }, \n",
    ")\n",
    "print(response_create_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transform output\")\n",
    "#Display the content of S3 Bucket\n",
    "# List the objects in the bucket\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "for obj in response['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_boto3.delete_model(ModelName=ModelName)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "aws-marketplace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
